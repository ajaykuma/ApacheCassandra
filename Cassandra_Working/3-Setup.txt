GKE standard cluster setup
-------------------
ak7singhal@cloudshell:~ (cryptic-tower-477608-g6)$ kubectl get namespaces
NAME                          STATUS   AGE
default                       Active   10m
gke-managed-cim               Active   9m21s
gke-managed-system            Active   9m5s
gke-managed-volumepopulator   Active   8m59s
gmp-public                    Active   8m40s
gmp-system                    Active   8m40s
kube-node-lease               Active   10m
kube-public                   Active   10m
kube-system                   Active   10m

ak7singhal@cloudshell:~ (cryptic-tower-477608-g6)$ kubectl get nodes -o wide -n default
NAME                                       STATUS   ROLES    AGE     VERSION               INTERNAL-IP   EXTERNAL-IP       OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME
gke-cluster-1-default-pool-a36610f4-3vr3   Ready    <none>   9m9s    v1.33.5-gke.1201000   10.128.0.13   34.121.132.79     Ubuntu 24.04.3 LTS   6.8.0-1037-gke   containerd://2.0.4
gke-cluster-1-default-pool-a36610f4-hrgv   Ready    <none>   9m4s    v1.33.5-gke.1201000   10.128.0.16   34.173.182.94     Ubuntu 24.04.3 LTS   6.8.0-1037-gke   containerd://2.0.4
gke-cluster-1-default-pool-be2ed74c-tf2q   Ready    <none>   9m10s   v1.33.5-gke.1201000   10.128.0.11   35.239.254.63     Ubuntu 24.04.3 LTS   6.8.0-1037-gke   containerd://2.0.4
gke-cluster-1-default-pool-be2ed74c-z18l   Ready    <none>   9m9s    v1.33.5-gke.1201000   10.128.0.12   136.114.206.109   Ubuntu 24.04.3 LTS   6.8.0-1037-gke   containerd://2.0.4
gke-cluster-1-default-pool-dc143d77-bq4p   Ready    <none>   9m4s    v1.33.5-gke.1201000   10.128.0.15   136.115.122.111   Ubuntu 24.04.3 LTS   6.8.0-1037-gke   containerd://2.0.4
gke-cluster-1-default-pool-dc143d77-hqcg   Ready    <none>   9m6s    v1.33.5-gke.1201000   10.128.0.14   35.224.250.204    Ubuntu 24.04.3 LTS   6.8.0-1037-gke   containerd://2.0.4

ak7singhal@cloudshell:~ (cryptic-tower-477608-g6)$ gcloud container node-pools list --cluster cluster-1 --region us-central1
NAME: default-pool
MACHINE_TYPE: n1-standard-2
DISK_SIZE_GB: 32
NODE_VERSION: 1.33.5-gke.1201000

ak7singhal@cloudshell:~ (cryptic-tower-477608-g6)$ kubectl get pods -A
NAMESPACE         NAME                                                  READY   STATUS    RESTARTS   AGE
gke-managed-cim   kube-state-metrics-0                                  2/2     Running   0          11m
gmp-system        collector-ggkwk                                       2/2     Running   0          10m
gmp-system        collector-h8qm8                                       2/2     Running   0          10m
gmp-system        collector-kk59q                                       2/2     Running   0          10m
gmp-system        collector-kqjt9                                       2/2     Running   0          10m
gmp-system        collector-nqs7g                                       2/2     Running   0          10m
gmp-system        collector-scm8r                                       2/2     Running   0          10m
gmp-system        gmp-operator-848d89cfcb-smtzr                         1/1     Running   0          10m
kube-system       event-exporter-gke-5746bc5dbd-gbfwp                   2/2     Running   0          10m
kube-system       fluentbit-gke-2xwrj                                   3/3     Running   0          10m
kube-system       fluentbit-gke-dbwhs                                   3/3     Running   0          10m
kube-system       fluentbit-gke-frh8p                                   3/3     Running   0          10m
kube-system       fluentbit-gke-nglvt                                   3/3     Running   0          10m
kube-system       fluentbit-gke-ttcpk                                   3/3     Running   0          10m
kube-system       fluentbit-gke-vvm5h                                   3/3     Running   0          10m
kube-system       gke-metrics-agent-8jr46                               3/3     Running   0          10m
kube-system       gke-metrics-agent-9hbkz                               3/3     Running   0          10m
kube-system       gke-metrics-agent-g6b6l                               3/3     Running   0          10m
kube-system       gke-metrics-agent-hx8pv                               3/3     Running   0          10m
kube-system       gke-metrics-agent-jfwq6                               3/3     Running   0          10m
kube-system       gke-metrics-agent-nb5qr                               3/3     Running   0          10m
kube-system       konnectivity-agent-59f676f7df-4pzdh                   2/2     Running   0          10m
kube-system       konnectivity-agent-59f676f7df-5jq4v                   2/2     Running   0          10m
kube-system       konnectivity-agent-59f676f7df-82jnl                   2/2     Running   0          11m
kube-system       konnectivity-agent-59f676f7df-pgkqx                   2/2     Running   0          10m
kube-system       konnectivity-agent-59f676f7df-qcvx2                   2/2     Running   0          10m
kube-system       konnectivity-agent-59f676f7df-wcnwl                   2/2     Running   0          10m
kube-system       konnectivity-agent-autoscaler-6d9956645f-gms8f        1/1     Running   0          11m
kube-system       kube-dns-7bf5cb79f7-dlcdh                             4/4     Running   0          10m
kube-system       kube-dns-7bf5cb79f7-qrwp9                             4/4     Running   0          11m
kube-system       kube-dns-autoscaler-5bf7f79bb4-x8k4n                  1/1     Running   0          11m
kube-system       kube-proxy-gke-cluster-1-default-pool-a36610f4-3vr3   1/1     Running   0          10m
kube-system       kube-proxy-gke-cluster-1-default-pool-a36610f4-hrgv   1/1     Running   0          10m
kube-system       kube-proxy-gke-cluster-1-default-pool-be2ed74c-tf2q   1/1     Running   0          10m
kube-system       kube-proxy-gke-cluster-1-default-pool-be2ed74c-z18l   1/1     Running   0          10m
kube-system       kube-proxy-gke-cluster-1-default-pool-dc143d77-bq4p   1/1     Running   0          10m
kube-system       kube-proxy-gke-cluster-1-default-pool-dc143d77-hqcg   1/1     Running   0          10m
kube-system       l7-default-backend-78858cccc9-p2xnr                   1/1     Running   0          11m
kube-system       metrics-server-v1.33.0-549f58f67d-jdhb5               1/1     Running   0          9m12s
kube-system       pdcsi-node-42ldb                                      2/2     Running   0          10m
kube-system       pdcsi-node-47ftx                                      2/2     Running   0          10m
kube-system       pdcsi-node-6slfz                                      2/2     Running   0          10m
kube-system       pdcsi-node-rsrhc                                      2/2     Running   0          10m
kube-system       pdcsi-node-tw28k                                      2/2     Running   0          10m
kube-system       pdcsi-node-w6fg4                                      2/2     Running   0          10m

#Install cert-manager
ak7singhal@cloudshell:~ (cryptic-tower-477608-g6)$ helm repo add jetstack https://charts.jetstack.io
helm repo update

helm install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --version v1.13.2 \
  --set installCRDs=true

--output---
"jetstack" has been added to your repositories
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "jetstack" chart repository
Update Complete. ⎈Happy Helming!⎈

NAME: cert-manager
LAST DEPLOYED: Sat Nov 29 21:16:56 2025
NAMESPACE: cert-manager
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
cert-manager v1.13.2 has been deployed successfully!

In order to begin issuing certificates, you will need to set up a ClusterIssuer
or Issuer resource (for example, by creating a 'letsencrypt-staging' issuer).

More information on the different types of issuers and how to configure them
can be found in our documentation:

https://cert-manager.io/docs/configuration/

For information on how to configure cert-manager to automatically provision
Certificates for Ingress resources, take a look at the `ingress-shim`
documentation:

https://cert-manager.io/docs/usage/ingress/

--output ends--

ak7singhal@cloudshell:~ (cryptic-tower-477608-g6)$ kubectl get pods -n cert-manager
NAME                                       READY   STATUS    RESTARTS   AGE
cert-manager-597f9759f-d7v8w               1/1     Running   0          118s
cert-manager-cainjector-6dc4d85566-xjwcs   1/1     Running   0          118s
cert-manager-webhook-6787b9c856-8xtcl      1/1     Running   0          118s

#Install K8ssandra Operator

helm repo add k8ssandra https://helm.k8ssandra.io/
helm repo update

helm install k8ssandra-operator k8ssandra/k8ssandra-operator \
  --namespace k8ssandra-operator \
  --create-namespace

---output---
"k8ssandra" has been added to your repositories
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "jetstack" chart repository
...Successfully got an update from the "k8ssandra" chart repository
Update Complete. ⎈Happy Helming!⎈
NAME: k8ssandra-operator
LAST DEPLOYED: Sat Nov 29 21:20:36 2025
NAMESPACE: k8ssandra-operator
STATUS: deployed
REVISION: 1
TEST SUITE: None

----output ends-----

kubectl get pods -n k8ssandra-operator

kubectl get pods -n k8ssandra-operator
NAME                                                READY   STATUS    RESTARTS   AGE
k8ssandra-operator-7776cd85b4-9m6tj                 1/1     Running   0          53s
k8ssandra-operator-cass-operator-76db5985c7-j88cd   1/1     Running   0          53s

ak7singhal@cloudshell:~ (cryptic-tower-477608-g6)$ kubectl get storageclass
NAME                     PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
premium-rwo              pd.csi.storage.gke.io   Delete          WaitForFirstConsumer   true                   24m
standard                 kubernetes.io/gce-pd    Delete          Immediate              true                   24m
standard-rwo (default)   pd.csi.storage.gke.io   Delete          WaitForFirstConsumer   true                   24m

mkdir k8ssandra

cd k8ssandra

vi k8ssandra.yaml

apiVersion: k8ssandra.io/v1alpha1
kind: K8ssandraCluster
metadata:
  name: k8ssandra-example
  namespace: k8ssandra-operator
spec:
  cassandra:
    serverType: cassandra
    serverVersion: "4.0.11"
    datacenters:
      - metadata:
          name: dc1
        size: 3
        resources:
          requests:
            cpu: "2"
            memory: "4Gi"
          limits:
            cpu: "2"
            memory: "4Gi"

        racks:
          - name: default

        storageConfig:
          cassandraDataVolumeClaimSpec:
            storageClassName: standard-rwo
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 10Gi

        config:
          cassandraYaml:
            memtable_allocation_type: heap_buffers
          jvmOptions:
            heapSize: "2Gi"
            heapNewGenSize: "512Mi"


kubectl apply -f k8ssandra.yaml 
k8ssandracluster.k8ssandra.io/k8ssandra-example created

kubectl get cassdc -n k8ssandra-operator
NAME   AGE
dc1    34s

--watch pods coming up
kubectl get pods -n k8ssandra-operator -w

kubectl get pods -n k8ssandra-operator -w
NAME                                                READY   STATUS    RESTARTS   AGE
k8ssandra-example-dc1-default-sts-0                 2/2     Running   0          18m
k8ssandra-example-dc1-default-sts-1                 2/2     Running   0          18m
k8ssandra-example-dc1-default-sts-2                 0/2     Pending   0          90s
k8ssandra-operator-7776cd85b4-9m6tj                 1/1     Running   0          23m
k8ssandra-operator-cass-operator-76db5985c7-j88cd   1/1     Running   0          23m

Issue with 3rd POD, can be seen
ak7singhal@cloudshell:~/k8ssandra (cryptic-tower-477608-g6)$ kubectl describe pod k8ssandra-example-dc1-default-sts-2 -n k8ssandra-operator
Name:             k8ssandra-example-dc1-default-sts-2
Namespace:        k8ssandra-operator
Priority:         0
Service Account:  default
Node:             <none>
Labels:           app.kubernetes.io/created-by=cass-operator
                  app.kubernetes.io/instance=cassandra-k8ssandra-example
                  app.kubernetes.io/managed-by=cass-operator
                  app.kubernetes.io/name=cassandra
                  app.kubernetes.io/version=4.0.11
                  apps.kubernetes.io/pod-index=2
                  cassandra.datastax.com/cluster=k8ssandra-example
                  cassandra.datastax.com/datacenter=dc1
                  cassandra.datastax.com/node-state=Ready-to-Start
                  cassandra.datastax.com/rack=default
                  controller-revision-hash=k8ssandra-example-dc1-default-sts-7d7f6f5fdd
                  statefulset.kubernetes.io/pod-name=k8ssandra-example-dc1-default-sts-2
Annotations:      cloud.google.com/cluster_autoscaler_unhelpable_since: 2025-11-29T21:43:13+0000
                  cloud.google.com/cluster_autoscaler_unhelpable_until: Inf
Status:           Pending
IP:               
IPs:              <none>
Controlled By:    StatefulSet/k8ssandra-example-dc1-default-sts
Init Containers:
  server-config-init:
    Image:      docker.io/datastax/cass-config-builder:1.0-ubi
    Port:       <none>
    Host Port:  <none>
    Limits:
      cpu:     1
      memory:  384M
    Requests:
      cpu:     1
      memory:  256M
    Environment:
      POD_IP:                      (v1:status.podIP)
      HOST_IP:                     (v1:status.hostIP)
      USE_HOST_IP_FOR_BROADCAST:  false
      RACK_NAME:                  default
      PRODUCT_VERSION:            4.0.11
      PRODUCT_NAME:               cassandra
      POD_NAME:                   k8ssandra-example-dc1-default-sts-2 (v1:metadata.name)
      CONFIG_FILE_DATA:           {"cassandra-env-sh":{"additional-jvm-opts":["-Dcassandra.allow_alter_rf_during_range_movement=true","-Dcassandra.system_distributed_replication=dc1:3"]},"cassandra-yaml":{"authenticator":"PasswordAuthenticator","authorizer":"CassandraAuthorizer","memtable_allocation_type":"heap_buffers","num_tokens":16,"role_manager":"CassandraRoleManager"},"cluster-info":{"name":"k8ssandra-example","seeds":"k8ssandra-example-seed-service,k8ssandra-example-dc1-additional-seed-service"},"datacenter-info":{"graph-enabled":0,"name":"dc1","solr-enabled":0,"spark-enabled":0},"jvm-server-options":{"initial_heap_size":2147483648,"max_heap_size":2147483648},"jvm11-server-options":{"garbage_collector":"G1GC"}}
    Mounts:
      /config from server-config (rw)
      /opt/management-api/configs from metrics-agent-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9wjn2 (ro)
Containers:
  cassandra:
    Image:       docker.io/k8ssandra/cass-management-api:4.0.11-ubi
    Ports:       9042/TCP, 9142/TCP, 7000/TCP, 7001/TCP, 7199/TCP, 8080/TCP, 9103/TCP, 9000/TCP
    Host Ports:  0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP
    Limits:
      cpu:     2
      memory:  4Gi
    Requests:
      cpu:      2
      memory:   4Gi
    Liveness:   http-get http://:8080/api/v0/probes/liveness delay=15s timeout=10s period=15s #success=1 #failure=3
    Readiness:  http-get http://:8080/api/v0/probes/readiness delay=20s timeout=10s period=10s #success=1 #failure=3
    Environment:
      METRIC_FILTERS:           deny:org.apache.cassandra.metrics.Table deny:org.apache.cassandra.metrics.table allow:org.apache.cassandra.metrics.table.live_ss_table_count allow:org.apache.cassandra.metrics.Table.LiveSSTableCount allow:org.apache.cassandra.metrics.table.live_disk_space_used allow:org.apache.cassandra.metrics.table.LiveDiskSpaceUsed allow:org.apache.cassandra.metrics.Table.Pending allow:org.apache.cassandra.metrics.Table.Memtable allow:org.apache.cassandra.metrics.Table.Compaction allow:org.apache.cassandra.metrics.table.read allow:org.apache.cassandra.metrics.table.write allow:org.apache.cassandra.metrics.table.range allow:org.apache.cassandra.metrics.table.coordinator allow:org.apache.cassandra.metrics.table.dropped_mutations
      POD_NAME:                 k8ssandra-example-dc1-default-sts-2 (v1:metadata.name)
      NODE_NAME:                 (v1:spec.nodeName)
      DS_LICENSE:               accept
      USE_MGMT_API:             true
      MGMT_API_NO_KEEP_ALIVE:   true
      MGMT_API_EXPLICIT_START:  true
    Mounts:
      /config from server-config (rw)
      /opt/management-api/configs from metrics-agent-config (rw)
      /tmp from tmp (rw)
      /var/lib/cassandra from server-data (rw)
      /var/log/cassandra from server-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9wjn2 (ro)
  server-system-logger:
    Image:      docker.io/k8ssandra/system-logger:v1.28.0
    Port:       <none>
    Host Port:  <none>
    Limits:
      memory:  128M
    Requests:
      cpu:     100m
      memory:  64M
    Environment:
      POD_NAME:         k8ssandra-example-dc1-default-sts-2 (v1:metadata.name)
      NODE_NAME:         (v1:spec.nodeName)
      CLUSTER_NAME:     k8ssandra-example
      DATACENTER_NAME:  dc1
      RACK_NAME:         (v1:metadata.labels['cassandra.datastax.com/rack'])
      NAMESPACE:        k8ssandra-operator (v1:metadata.namespace)
    Mounts:
      /opt/management-api/configs from metrics-agent-config (rw)
      /var/lib/vector from vector-lib (rw)
      /var/log/cassandra from server-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9wjn2 (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  server-data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  server-data-k8ssandra-example-dc1-default-sts-2
    ReadOnly:   false
  server-config:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  server-logs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  vector-lib:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  metrics-agent-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      k8ssandra-example-dc1-metrics-agent-config
    Optional:  false
  kube-api-access-9wjn2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason             Age    From                Message
  ----     ------             ----   ----                -------
  Warning  FailedScheduling   4m34s  default-scheduler   0/6 nodes are available: 1 Too many pods, 6 Insufficient cpu. preemption: 0/6 nodes are available: 6 No preemption victims found for incoming pod.
  Normal   NotTriggerScaleUp  4m35s  cluster-autoscaler  Pod didn't trigger scale-up:

--to delete setup
kubectl delete k8ssandracluster k8ssandra-example -n k8ssandra-operator









