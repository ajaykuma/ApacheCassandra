==========================================================================
Working with NODETOOL

See 'nodetool help <command>' for more information on a specific command.
hdu@mh2:~$ nodetool help
usage: nodetool [(-u <username> | --username <username>)]
        [(-pw <password> | --password <password>)] [(-pp | --print-port)]
        [(-h <host> | --host <host>)] [(-p <port> | --port <port>)]
        [(-pwf <passwordFilePath> | --password-file <passwordFilePath>)] <command>
        [<args>]

The most commonly used nodetool commands are:
.....

===========
#Apache Cassandra
#nodetool status
The status command shows information about the entire cluster, particularly the state of each
node, and information about each of those nodes: IP address, data load, number of tokens,total percentage of data saved on each node, host ID, and datacenter and rack.

--in single node
ubuntu@ds201-node1:~/node/bin$ ./nodetool status
Datacenter: Cassandra

Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Owns    Host ID                               Token                                    Rack
UN  127.0.0.1  205.63 KiB  ?       971960f1-2598-455d-a451-c8808497204b  0                                        rack1

Note: Non-system keyspaces don't have the same replication settings, effective ownership information is meaningless

--in multi node
hdu@mh2:~$ nodetool status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load        Tokens  Owns (effective)  Host ID                               Rack 
UN  192.168.103.4  179.54 KiB  256     60.0%             bc99d2b2-1ab8-4b3d-91d1-73a6e92b5359  rack1
UN  192.168.103.5  119.92 KiB  256     74.0%             1280b649-f98a-4aa5-aed9-9a3adf26c496  rack1
UN  192.168.103.3  159.52 KiB  256     66.0%             6dcb46b7-b82d-4f09-84fc-026b90794bcf  rack1


#Datastax enterprise
ubuntu@ds201-node1:~/node/bin$ ./dsetool status
DC: Cassandra       Workload: Cassandra       Graph: no     
======================================================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--   Address          Load             Owns                 Token                                        Rack         Health [0,1] 
UN   127.0.0.1        205.63 KiB       ?                    0                                            rack1        0.00         

Note: you must specify a keyspace to get ownership information.
Take note as to the differences between dsetool status and nodetool status. Although
both tools have a status command, dsetool works with DataStax EnterpriseTM as a whole
(Apache CassandraTM, Apache SparkTM, Apache SolrTM, Graph) whereas nodetool is specific to
Apache CassandraTM.
============

#nodetool info
--in single node
ubuntu@ds201-node1:~/node/bin$ ./nodetool info
ID                     : 971960f1-2598-455d-a451-c8808497204b
Gossip active          : true
Native Transport active: true
Load                   : 205.63 KiB
Generation No          : 1590491970
Uptime (seconds)       : 380
Heap Memory (MB)       : 311.50 / 512.00
Off Heap Memory (MB)   : 0.00
Data Center            : Cassandra
Rack                   : rack1
Exceptions             : 0
Key Cache              : entries 0, size 0 bytes, capacity 25 MiB, 0 hits, 0 requests, NaN recent hit rate, 14400 save period in seconds
Row Cache              : entries 0, size 0 bytes, capacity 0 bytes, 0 hits, 0 requests, NaN recent hit rate, 0 save period in seconds
Counter Cache          : entries 0, size 0 bytes, capacity 12 MiB, 0 hits, 0 requests, NaN recent hit rate, 7200 save period in seconds
Chunk Cache            : entries 123, size 3.29 MiB, capacity 1.48 GiB, 123 misses, 911 requests, 0.865 recent hit rate, 1188.230 microseconds miss latency
Percent Repaired       : 100.0%
Token                  : 0

The info command displays information about the connected node, which includes token
information, host ID, protocol status, data load, node uptime, heap memory usage and
capacity, datacenter and rack information, number of errors reported, cache usage, and
percentage of SSTables that have been incrementally repaired.

--in multi node
hdu@mh2:~$ nodetool info
ID                     : bc99d2b2-1ab8-4b3d-91d1-73a6e92b5359
Gossip active          : true
Native Transport active: true
Load                   : 179.54 KiB
Generation No          : 1700615088
Uptime (seconds)       : 243
Heap Memory (MB)       : 161.09 / 1014.00
Off Heap Memory (MB)   : 0.00
Data Center            : datacenter1
Rack                   : rack1
Exceptions             : 0
Key Cache              : entries 10, size 896 bytes, capacity 50 MiB, 92 hits, 105 requests, 0.876 recent hit rate, 14400 save period in seconds
Row Cache              : entries 0, size 0 bytes, capacity 0 bytes, 0 hits, 0 requests, NaN recent hit rate, 0 save period in seconds
Counter Cache          : entries 0, size 0 bytes, capacity 25 MiB, 0 hits, 0 requests, NaN recent hit rate, 7200 save period in seconds
Network Cache          : size 8 MiB, overflow size: 0 bytes, capacity 63 MiB
Percent Repaired       : 100.0%
Token                  : (invoke with -T/--tokens to see all 256 tokens)

--in multi node cluster
hdu@mh2:~$ nodetool ring

Datacenter: datacenter1
==========
Address             Rack        Status State   Load            Owns                Token                                       
                                                                                   9212322661507734165                         
192.168.103.3       rack1       Up     Normal  159.52 KiB      65.95%              -9211980501134640027                        
192.168.103.5       rack1       Up     Normal  119.92 KiB      74.01%              -9190156010506808712                        
192.168.103.4       rack1       Up     Normal  179.54 KiB      60.04%              -9174170618174946117                        
192.168.103.3       rack1       Up     Normal  159.52 KiB      65.95%              -9144539666105850265                        
192.168.103.5       rack1       Up     Normal  119.92 KiB      74.01%              -9115642899194864773                        
192.168.103.3       rack1       Up     Normal  159.52 KiB      65.95%              -9098031998131860961                        
192.168.103.5       rack1       Up     Normal  119.92 KiB      74.01%              -9068621586794155506                        
192.168.103.4       rack1       Up     Normal  179.54 KiB      60.04%              -9046783616474764377
.............
192.168.103.3       rack1       Up     Normal  159.52 KiB      65.95%              9093416482341009847                         
192.168.103.5       rack1       Up     Normal  119.92 KiB      74.01%              9122465532444229692                         
192.168.103.4       rack1       Up     Normal  179.54 KiB      60.04%              9144119227343856112                         
192.168.103.3       rack1       Up     Normal  159.52 KiB      65.95%              9178677721486597802                         
192.168.103.4       rack1       Up     Normal  179.54 KiB      60.04%              9212322661507734165                         

  Warning: "nodetool ring" is used to output all the tokens of a node.
  To view status related info of a node use "nodetool status" instead.

--in single node

#nodetool describecluster
ubuntu@ds201-node1:~/node/bin$ ./nodetool describecluster
Cluster Information:
	Name: Test Cluster
	Snitch: com.datastax.bdp.snitch.DseDelegateSnitch
	DynamicEndPointSnitch: enabled
	Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
	Schema versions:
		e8ff6427-6a12-35a2-ba5c-7d1c1e9e98a6: [127.0.0.1]

describecluster shows the settings that are common across all of the nodes in the cluster
and the current schema version used by each node.

--in multi node
hdu@mh2:~$ nodetool describecluster
Cluster Information:
	Name: Test ClusterNp
	Snitch: org.apache.cassandra.locator.SimpleSnitch
	DynamicEndPointSnitch: enabled
	Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
	Schema versions:
		54e17321-3f2e-37ca-9b08-d91ba7bdd369: [192.168.103.3, 192.168.103.4, 192.168.103.5]

Stats for all nodes:
	Live: 3
	Joining: 0
	Moving: 0
	Leaving: 0
	Unreachable: 0

Data Centers: 
	datacenter1 #Nodes: 3 #Down: 0

Database versions:
	4.1.2: [192.168.103.3:7000, 192.168.103.4:7000, 192.168.103.5:7000]

Keyspaces:
	system_schema -> Replication class: LocalStrategy {}
	system -> Replication class: LocalStrategy {}
	system_auth -> Replication class: SimpleStrategy {replication_factor=1}
	system_distributed -> Replication class: SimpleStrategy {replication_factor=3}
	system_traces -> Replication class: SimpleStrategy {replication_factor=2}

--in single node
ubuntu@ds201-node1:~/node/bin$ ./nodetool getlogginglevels

Logger Name                                        Log Level
ROOT                                                    INFO
DroppedAuditEventLogger                                 INFO
SLF4JAuditWriter                                        INFO
com.cryptsoft                                            OFF
com.datastax.bdp.db                                    DEBUG
com.datastax.bdp.search.solr.metrics.SolrMetricsEventListener     DEBUG
com.datastax.driver.core.NettyUtil                     ERROR
org.apache.cassandra                                   DEBUG
org.apache.lucene.index                                 INFO
org.apache.solr.core.CassandraSolrConfig                WARN
org.apache.solr.core.RequestHandlers                    WARN
org.apache.solr.core.SolrCore                           WARN
org.apache.solr.handler.component                       WARN
org.apache.solr.search.SolrIndexSearcher                WARN
org.apache.solr.update                                  WARN
org.apache.spark.rpc                                   ERROR

--in multi node
hdu@mh2:~$ nodetool getlogginglevels

Logger Name                                        Log Level
ROOT                                                    INFO
org.apache.cassandra                                   DEBUG

====================
./nodetool setlogginglevel org.apache.cassandra TRACE
The command setlogginglevel dynamically changes the logging level used by
Apache CassandraTM without the need for a restart. You can also look at the
/var/log/cassandra/system.log afterwards to observe the changes.

===================
./nodetool settraceprobability 0.1
The resultant value from the settraceprobability command represents a decimal
describing the percentage of queries being saved, starting from 0 (0%) to 1 (100%). Saved traces
can then be viewed in the system_traces keyspace.

==================
./nodetool drain
The drain command stops writes from occurring on the node and flushes all data to disk.
Typically, this command may be run before stopping an Apache CassandraTM node.

==================
/nodetool stopdaemon
The stopdaemon command stops a node's execution. Wait for it to complete.
12) Restart your node by running:
/home/ubuntu/node/bin/dse cassandra
Wait for Apache CassandraTM to start before continuing.

==================
nodetool gossipinfo
--in multi node
hdu@mh2:~$ nodetool gossipinfo
/192.168.103.3
  generation:1700615082
  heartbeat:456
  LOAD:407:163350.0
  SCHEMA:13:54e17321-3f2e-37ca-9b08-d91ba7bdd369
  DC:9:datacenter1
  RACK:11:rack1
  RELEASE_VERSION:6:4.1.2
  NET_VERSION:2:12
  HOST_ID:3:6dcb46b7-b82d-4f09-84fc-026b90794bcf
  RPC_READY:41:true
  NATIVE_ADDRESS_AND_PORT:4:192.168.103.3:9042
  STATUS_WITH_PORT:19:NORMAL,-1052696625419917833
  SSTABLE_VERSIONS:7:big-nb
  TOKENS:18:<hidden>
/192.168.103.4
  generation:1700615088
  heartbeat:440
  STATUS:19:NORMAL,-1021166935752633034
  LOAD:395:183852.0
  SCHEMA:13:54e17321-3f2e-37ca-9b08-d91ba7bdd369
  DC:9:datacenter1
  RACK:11:rack1
  RELEASE_VERSION:6:4.1.2
  RPC_ADDRESS:5:192.168.103.4
  NET_VERSION:2:12
  HOST_ID:3:bc99d2b2-1ab8-4b3d-91d1-73a6e92b5359
  RPC_READY:32:true
  NATIVE_ADDRESS_AND_PORT:4:192.168.103.4:9042
  STATUS_WITH_PORT:18:NORMAL,-1021166935752633034
  SSTABLE_VERSIONS:7:big-nb
  TOKENS:17:<hidden>
/192.168.103.5
  generation:1700615103
  heartbeat:426
  LOAD:396:122796.0
  SCHEMA:13:54e17321-3f2e-37ca-9b08-d91ba7bdd369
  DC:9:datacenter1
  RACK:11:rack1
  RELEASE_VERSION:6:4.1.2
  NET_VERSION:2:12
  HOST_ID:3:1280b649-f98a-4aa5-aed9-9a3adf26c496
  RPC_READY:32:true
  NATIVE_ADDRESS_AND_PORT:4:192.168.103.5:9042
  STATUS_WITH_PORT:18:NORMAL,-1108277113667924192
  SSTABLE_VERSIONS:7:big-nb
  TOKENS:17:<hidden>

nodetool gettimeout read
nodetool gettimeout write

--in multi node
hdu@mh2:~$ nodetool gettimeout read
Current timeout for type read: 5000 ms
hdu@mh2:~$ nodetool gettimeout write
Current timeout for type write: 2000 ms


nodetool version
==============
We will now stress the node using a simple tool called Apache Cassandra(TM) Stress. 
Once your node has restarted, navigate to the
/home/ubuntu/node/resources/cassandra/tools/bin directory in the terminal. 

Run cassandra-stress to populate the cluster with 50,000 partitions using 1 client thread and
without any warmup using:
./cassandra-stress write n=50000 no-warmup -rate threads=1

=====
ubuntu@ds201-node1:~/node/resources/cassandra/tools/bin$ ./cassandra-stress write n=50000 no-warmup -rate threads=1
******************** Stress Settings ********************
Command:
  Type: write
  Count: 50,000
  No Warmup: true
  Consistency Level: LOCAL_ONE
  Target Uncertainty: not applicable
  Key Size (bytes): 10
  Counter Increment Distibution: add=fixed(1)
Rate:
  Auto: false
  Thread Count: 1
  OpsPer Sec: 0
Population:
  Sequence: 1..50000
  Order: ARBITRARY
  Wrap: true
Insert:
  Revisits: Uniform:  min=1,max=1000000
  Visits: Fixed:  key=1
  Row Population Ratio: Ratio: divisor=1.000000;delegate=Fixed:  key=1
  Batch Type: not batching
Columns:
  Max Columns Per Key: 5
  Column Names: [C0, C1, C2, C3, C4]
  Comparator: AsciiType
  Timestamp: null
  Variable Column Count: false
  Slice: false
  Size Distribution: Fixed:  key=34
  Count Distribution: Fixed:  key=5
Errors:
  Ignore: false
  Tries: 10
Log:
  No Summary: false
  No Settings: false
  No Progress: false
  Show Queries: false
  Query Log File: null
  File: null
  Interval Millis: 1000
  Level: NORMAL
Mode:
  API: JAVA_DRIVER_NATIVE
  Connection Style: CQL_PREPARED
  CQL Version: CQL3
  Protocol Version: DSE_V2
  Username: null
  Password: null
  Auth Provide Class: null
  Max Pending Per Connection: 128
  Connections Per Host: 8
  Compression: NONE
Node:
  Nodes: [localhost]
  Is White List: false
  Datacenter: null
Schema:
  Keyspace: keyspace1
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Replication Strategy Options: {replication_factor=1}
  Table Compression: null
  Table Compaction Strategy: null
  Table Compaction Strategy Options: {}
Transport:
  truststore=null; truststore-password=null; keystore=null; keystore-password=null; ssl-protocol=TLS; ssl-alg=SunX509; ssl-ciphers=TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA; 
Port:
  Native Port: 9042
  JMX Port: 7199
Send To Daemon:
  *not set*
Graph:
  File: null
  Revision: unknown
  Title: null
  Operation: WRITE
TokenRange:
  Wrap: true
  Split Factor: 1

Connected to cluster: Test Cluster, max pending requests per connection 128, max connections per host 8
Datacenter: Cassandra; Host: localhost/127.0.0.1; Rack: rack1
Created keyspaces. Sleeping 1s for propagation.
Sleeping 2s...
Running WRITE with 1 threads for 50000 iteration
type       total ops,    op/s,    pk/s,   row/s,    mean,     med,     .95,     .99,    .999,     max,   time,   stderr, errors,  gc: #,  max ms,  sum ms,  sdv ms,      mb
total,             3,       3,       3,       3,  16.900,   4.760,  41.943,  41.943,   41.94,    41.9,    1.0,  0.00000,      0,      0,       0,       0,       0,       0
total,           330,     327,     327,     327,   2.908,   2.019,   7.688,  14.180,   27.08,    27.1,    2.0,  0.69405,      0,      0,       0,       0,       0,       0
total,           788,     458,     458,     458,   2.131,   1.417,   6.533,  10.568,   19.71,    19.7,    3.0,  0.41955,      0,      0,       0,       0,       0,       0
total,          1227,     439,     439,     439,   2.224,   1.574,   5.616,  11.534,   23.59,    23.6,    4.0,  0.29681,      0,      0,       0,       0,       0,       0
total,          1925,     698,     698,     698,   1.402,   0.864,   3.715,   8.360,   13.97,    18.7,    5.0,  0.26179,      0,      0,       0,       0,       0,       0
total,         50000,    3979,    3979,    3979,   0.239,   0.173,   0.357,   2.093,    4.08,     4.1,   32.1,  0.13210,      0,      0,       0,       0,       0,       0


Results:
Op rate                   :    1,556 op/s  [WRITE: 1,556 op/s]
Partition rate            :    1,556 pk/s  [WRITE: 1,556 pk/s]
Row rate                  :    1,556 row/s [WRITE: 1,556 row/s]
Latency mean              :  0.603 ms [WRITE: 0.6 ms]
Latency median            :  0.303 ms [WRITE: 0.3 ms]
Latency 95th percentile   :  2.041 ms [WRITE: 2.0 ms]
Latency 99th percentile   :  4.977 ms [WRITE: 5.0 ms]
Latency 99.9th percentile : 13.967 ms [WRITE: 14.0 ms]
Latency max               : 89.915 ms [WRITE: 89.9 ms]
Total partitions          :     50,000 [WRITE: 50,000]
Total errors              :          0 [WRITE: 0]
Total GC count            : 3
Total GC memory           : 863.249 MiB
Total GC time             :    0.1 seconds
Avg GC time               :   39.3 ms
StdDev GC time            :   10.9 ms
Total operation time      : 00:00:32

END

=====
Initially, we will see a long list of setting for the stress run. As Apache CassandraTM stress
executes, it logs several statistics to the terminal. Each line displays the statistics for the
operations that occurred each second and shows number of partitions written, operations per
second, latency information, and more.

Navigate back to /home/ubuntu/node/resources/cassandra/bin and run:
./nodetool flush
The flush command commits all written (memtable, discussed later) data to disk. Unlike
drain, flush allows further writes to occur.

Check the new load on the node. Run:./nodetool status
===========
ubuntu@ds201-node1:~/node/resources/cassandra/bin$ ./nodetool flush
ubuntu@ds201-node1:~/node/resources/cassandra/bin$ ./nodetool status
Datacenter: Cassandra

=====================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Owns    Host ID                               Token                                    Rack
UN  127.0.0.1  11.61 MiB  ?       971960f1-2598-455d-a451-c8808497204b  0                                        rack1

Note: Non-system keyspaces don't have the same replication settings, effective ownership information is meaningless
=====================

We will now examine the data cassandra-stress wrote to our node. Start ./cqlsh.
Execute the following CQLSH command to view the current keyspaces:
DESCRIBE KEYSPACES;
Notice the presence of keyspace1 which cassandra-stress created.

Switch to that keyspace by executing the following:
USE keyspace1;

View the tables in keyspace1 by executing the following:
DESCRIBE TABLES;

Query the first five rows from standard1 by executing the following query:
SELECT *
FROM standard1
LIMIT 5;
The data that was written is not very meaningful, since they are all arbitrary BLOB values.

========
cqlsh> describe keyspaces;

dse_system_local  system_schema  dse_leases          system_traces  dse_system
dse_security      system_auth    keyspace1           dse_perf     
solr_admin        system         system_distributed  killrvideo   

cqlsh> use keyspace1;
cqlsh:keyspace1> describe tables;

counter1  standard1

cqlsh:keyspace1> select * from standard1 limit 5;

 key                    | C0                                                                     | C1                                                                     | C2                                                                     | C3                                                                     | C4
------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------
 0x35384d4d334f39355030 | 0x2b021ed2ca95d8d6d450774953900af7226f692a1951fbb58099d51b54b9975a0022 | 0x3411457464d3cd9c716cad05cfc4a8a9556db308ec450e80afac7cbb2ee7a2e1040f | 0xb0b01f5612908837eeaaab017ec6f2cc988989916c02038e813f2b2811e2e5ffde35 | 0x7e7e764b144db561aa6722b593fb872cb41126362eebeeb233ba0dd97b9c761807ab | 0xb601e0fbd8b342baaae29bd469a24c906d960d36e80c5ddf2253b91335453833eec0
 0x314d324d31374b393431 | 0x1deda456d4ebf65ffe2cf623e8592d5621d9c83430add55fe2ac5cad67689ea54692 | 0x5f5b73a37e538de3bd766072e1032566bfa365c54d165745d8e81bb68fd6c1e0b70d | 0xa90f84cffb8601404d4be21c7d0f95da832b853852d112234803457ca87d59d96e4f | 0x612a8888062268cb811adbb591b4f9e7dceacad7c38c90af38d0e8501d76e26cab23 | 0xe50417a72c78b64e3ca433471b91b16651c25870fdc93b673552ce65da51480ddf6c
 0x33344b50353133365030 | 0xd34820f1e930d9257e02158146c64ea6b052903710919294f196c174591318a00c1a | 0x6becf1098ae914e65c0325081a8a4418f9d65575a831db087102140b8b52db166ef0 | 0x34b89becc7ae1d391e814edc105e7f7e5efa495e2ad4850a753bc5236902f6380690 | 0x02e517fcc81da646fc7ba6f9f5dd88a4012e33e977d055a4ea50a7c25da9ba19f6ea | 0x5692948b8be2c35ed8af2e40135e26fddb7af2f425374cf835622e20a74cf3655e80
 0x324d4f3031314c303330 | 0xc6d16d17a30611dc7646de3463be469dc56130a24345063854c356853a9966be1f01 | 0xbfdd8fd17ec738056feb9c176f64c5f0532b672dc398cd6d3470f05fc3aae437d990 | 0xc1cc57455dfa0cc0dd595accfa3751f6d5edac2e11b4e763ea88ca2b7f45ec77ab8d | 0x7998ea0ed12980933317b639d9b92ee20d0e6d66b1a531bd75c2c367aeeee47b73d4 | 0x9142cb1c0668c414c25eb264e52752f5e2cd86f3b15b3c5f5d8413f88fbcffbda7d2
 0x30303138344f4e503530 | 0x37e76a8c71afc1adda7958aa1182c5c100f120d91ced0323f3cc5090319fda464bf3 | 0x1a3a594716dc09025772fcf56724177110faa26f24e66a38cae64be7a568a182b9c9 | 0xcae650315e85148d46c91c857bc8a87e767fdb6e0450bee503f556bc1999a1dfeab5 | 0x6b48adf2bc3f36e8a1adb1314f41f28f0fbb69d2551f0b0e650f6595097af61c99a1 | 0xf8f02decf64b8889cbd28a01080b368849f065889b5de4addd73dc78c4343872a5c0
